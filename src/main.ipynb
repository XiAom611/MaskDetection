{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ac97edc-dc09-42ee-895b-4954dabc5aba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Face Mask Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba416cf3-1c7a-4cd4-9f3d-cb2335b8a442",
   "metadata": {},
   "source": [
    "Dataset\n",
    "https://www.kaggle.com/datasets/andrewmvd/face-mask-detection?datasetId=667889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76ac451-0019-42c8-9f47-eadf182dcf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import os\n",
    "import sys\n",
    "import xmltodict\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4505bed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd469b8-7385-462e-b04d-deb8fb846ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227016c3-d57b-4c96-b467-42cc59ec6fdd",
   "metadata": {},
   "source": [
    "## Read input images and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c380018-fa9d-4fb0-b8ca-3d089b85e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATASET = '../dataset/'\n",
    "PATH_TO_IMAGE = \"../dataset/images\"\n",
    "PATH_TO_ANNOTATION = \"../dataset/annotations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5b92a8-cb0d-4916-af80-4fbcf5613afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_names = []\n",
    "xml_names = []\n",
    "for dirpath, dirnames, filenames in os.walk(PATH_TO_DATASET):\n",
    "    for filename in filenames:\n",
    "        if filename[0] == \".\":\n",
    "            continue\n",
    "        elif filename[-3:] == \"xml\":\n",
    "            xml_names.append(os.path.join(dirpath, filename))\n",
    "        else:\n",
    "            img_names.append(os.path.join(dirpath, filename))\n",
    "# xml_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ecbb9-3f69-4ab2-9859-e5d5e6cbbb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = []\n",
    "for xml_name in xml_names:\n",
    "    with open(xml_name) as f:\n",
    "        res = xmltodict.parse(f.read())\n",
    "#         print(res)\n",
    "    annotation = res[\"annotation\"][\"object\"]\n",
    "#     print(xml_name, type(annotation))\n",
    "    if type(annotation) == list:\n",
    "#         print(annotation)\n",
    "        for i in range(len(annotation)):\n",
    "#             print(annotation[i][\"name\"])\n",
    "#             print(annotation[i]['bndbox'])\n",
    "            raw_data.append(annotation[i][\"name\"])\n",
    "    else:\n",
    "#         print(annotation[\"name\"])\n",
    "        raw_data.append(annotation[\"name\"])\n",
    "    \n",
    "# print(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13716c3f-e812-4915-b5a6-892d6b93fffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = Counter(raw_data).keys()\n",
    "count = Counter(raw_data).values()\n",
    "print(labels, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7343e6-689f-440c-a9ef-51fbccdfb52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize =(14,6))\n",
    "\n",
    "ax1.pie(count, wedgeprops=dict(width=0.3, edgecolor='w') ,\n",
    "        labels=labels, radius=1, startangle = 120, autopct='%1.2f%%')\n",
    "\n",
    "ax2 = plt.bar(labels, list(count),\n",
    "              color ='maroon',width = 0.4)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de05365a-a70a-4751-85fa-d5f3127157ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_ANNOTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b6be62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb5c01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(annotation,ax):\n",
    "    # set bbx edge colors\n",
    "    framecolor={'with_mask':'g','without_mask':'r','mask_weared_incorrect':'r'}\n",
    "    \n",
    "    xmin,ymin,xmax,ymax=list(map(int,annotation['bndbox'].values()))\n",
    "    #[('xmin', '221'), ('ymin', '101'), ('xmax', '256'), ('ymax', '139')]\n",
    "    \n",
    "    rec=Rectangle((xmin,ymin),xmax-xmin,ymax-ymin,\n",
    "                  linewidth=4,edgecolor=framecolor[annotation['name']],facecolor='none')\n",
    "    return rec\n",
    "    \n",
    "    \n",
    "def face_cas(img):\n",
    "    with open(PATH_TO_ANNOTATION+'/'+img[:-4]+'.xml') as f: # change png to xml\n",
    "        res=xmltodict.parse(f.read())\n",
    "        \n",
    "    # print(type(res[\"annotation\"][\"object\"]))\n",
    "        \n",
    "    image=plt.imread(os.path.join(PATH_TO_IMAGE, img))\n",
    "    annotation=res[\"annotation\"][\"object\"]\n",
    "    \n",
    "    fig,ax=plt.subplots(1,figsize=(14,6))\n",
    "    ax.axis('off')\n",
    "    if type(annotation)==list:\n",
    "        for i in range(len(annotation)):\n",
    "            # print(i)\n",
    "            rec=draw(annotation[i],ax)\n",
    "            ax.add_patch(rec)\n",
    "    else:\n",
    "        rec=draw(annotation,ax)\n",
    "        ax.add_patch(rec)\n",
    "    ax.imshow(image)\n",
    "\n",
    "img_names_indi=[]   #extract the image name individually\n",
    "for i in img_names:\n",
    "    img_names_indi.append(i.split('/')[-1])\n",
    "# print(fun_images)\n",
    "\n",
    "for i in range(10):\n",
    "    face_cas(img_names_indi[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4416e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes={'with_mask':1,\"withou_mask\":0,'mask_weared_incorrect':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a2389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans(annotation,it,lt,j):\n",
    "    xmin,ymin,xmax,ymax=list(map(int,annotation['bndbox'].values()))\n",
    "    #crop the image\n",
    "    image=transforms.functional.crop(Image.open(PATH_TO_IMAGE+'/'+j).convert('RGB'),\n",
    "                                     ymin,xmin,ymax-ymin,xmax-xmin)\n",
    "    \n",
    "    transformMethod=transforms.Compose([transforms.Resize((200,200)),\n",
    "                                      transforms.ToTensor()])\n",
    "    it.append(transformMethod(image))\n",
    "    lt.append(torch.tensor(classes[annotation['name']]))\n",
    "    \n",
    "def create_dataset(img_list):\n",
    "    image_tensor=[]\n",
    "    label_tensor=[]\n",
    "    for i in img_list:\n",
    "        with open(PATH_TO_ANNOTATION+'/'+i[:-4]+'.xml') as f:\n",
    "            res=xmltodict.parse(f.read())\n",
    "        annotation=res[\"annotation\"][\"object\"]\n",
    "    trans(annotation,image_tensor,label_tensor,i)\n",
    "    \n",
    "    dataset=[[i,j] for i, j in zip(image_tensor,label_tensor)]\n",
    "    return dataset\n",
    "        \n",
    "dataset=create_dataset(img_names_indi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce721ee8-5a9c-43a6-be5e-7db70d0560a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79858e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "print(f\"Dataset Length: {len(dataset)}, train_size: {train_size}, test_size: {test_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d69a4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a443dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23451ff2-8c64-443c-bcbd-c6175e902861",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e5502-c92f-46ac-9148-06b15503b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Model Parameters, attribute requires_grad to false when feature extraction is done.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a134c71-7d2f-40be-b97e-9986ec5bda74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e0c12-1dd4-4b22-8b96-cff24e2a8a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "n_inputs=model.fc.in_features\n",
    "last_layer=nn.Linear(n_inputs, 2)\n",
    "\n",
    "model.fc.out_features=last_layer\n",
    "print('reinitialize model with output features as 2 :', model.fc.out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b3547-9dbd-4036-abff-00b8ef0258a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_resnet50 = []\n",
    "for key,value in model._modules.items():\n",
    "    features_resnet50.append(value)\n",
    "\n",
    "features_resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e035f7-9a6f-4ab7-a580-d500208eb13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.SGD(model.parameters(),lr=0.001,momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f7675a-c9b6-4e4d-943e-53947bdc4557",
   "metadata": {},
   "outputs": [],
   "source": [
    "param.requires_grad=True\n",
    "ct = 0\n",
    "for child in model.children():\n",
    "    ct += 1\n",
    "    if ct < 7:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fe8204-f7bc-4b06-98e5-80f49997e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1,2): \n",
    "    running_loss = 0.0\n",
    "    train_losses = []\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            inputs , labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "        #inputs = inputs.to(device)\n",
    "        #labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() \n",
    "        if i % 20 == 19:    \n",
    "                \n",
    "                print(\"Epoch {}, batch {}, training loss {}\".format(epoch, i+1,running_loss/20))\n",
    "        \n",
    "        running_loss = 0.0\n",
    "     \n",
    "    \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e702d000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9234b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d888683a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3833f665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06253fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff33e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67dc93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b738a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4397e274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd771a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916afa56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc9239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "conda_venv",
   "language": "python",
   "name": "conda_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
